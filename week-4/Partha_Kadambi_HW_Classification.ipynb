{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 4 - Classification\n",
    "\n",
    "This week, we shift from gathering human textual classifications through crowdsourcing, to using machine learning models and algorithms that train on those human classifications and extend them to documents far too numerous to read. If you recall, *clustering* allows us to stably partition text data (e.g., documents, turns of conversation) according to all patterns of covariation among available text features. *Classification*, by contrast, partitions text data according to only those features and their variation that enable us to mimic and extrapolate human annotations.\n",
    "\n",
    "In this notebook, we will show how to use a variety of classification methods, including Na√Øve Bayes, Logistic regression, K-nearest neighbor, decision trees and random forests, support vector machines and even a simple neural network, the perceptron. We will also demonstrate ensemble techniques that can link several such methods into a single, more accurate, classification pipeline. We will finally learn to use conventions and metrics to evaluate classifier performance on out-of-sample data.\n",
    "\n",
    "For this notebook we will be using the following packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For ML\n",
    "import lucem_illud_2020\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.decomposition\n",
    "\n",
    "import numpy as np #arrays\n",
    "import matplotlib.pyplot as plt #Plots\n",
    "import matplotlib.colors # For nice colours\n",
    "import seaborn #Makes plots look nice, also heatmaps\n",
    "import scipy as sp #for interp\n",
    "\n",
    "#These are from the standard library\n",
    "import collections\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import glob\n",
    "import pandas\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Simulated Examples\n",
    "\n",
    "Here we create a sandbox for you to explore different types of classified data and how different statistical classifiers perform on each type."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating example data\n",
    "\n",
    "We start by loading one of the \"cartoon\" or simplified data sets and then dividing it into training and testing sets. To maximize our ability to visualize, each dataset involves two classes, colored yellow and blue, arrayed along two dimensions (`x` and `y`).\n",
    "\n",
    "The four data patterns include:\n",
    "+ `random` in which the two classes are randomly distributed across both dimensions\n",
    "+ `andSplit` in which the two classes are linearly split along one of two dimensions (e.g., men like Adidas)\n",
    "+ `xorSplit` in which the two classes are split, oppositely, along each dimension (e.g., old ladies and young men like Nikes)\n",
    "+ `targetSplit` in which one class is nested within the other in two dimensions (e.g., middle aged, middle income people like vintage Mustangs)\n",
    "+ `multiBlobs` in which 5 classes are placed as bivariate Gaussians at random locations\n",
    "\n",
    "`noise` is a variable [0-1] that ranges from no noise in the prescribed pattern [0] to complete noise/randomness [1].\n",
    "\n",
    "Uncomment (remove the # in front of) each dataset, one at a time, and then run the cell and subsequent cells to examine how each machine learning approach captures each pattern."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise = .2\n",
    "\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.random())\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.andSplit(noise))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.xorSplit(noise)) #Please try this one\n",
    "dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.targetSplit(noise))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.multiBlobs(noise))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating 10 new artificial datasets for Q1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise5 = .5\n",
    "\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.andSplit(noise5))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.xorSplit(noise5))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.targetSplit(noise5))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.multiBlobs(noise5))\n",
    "\n",
    "noise4 = .4\n",
    "\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.andSplit(noise4))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.xorSplit(noise4))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.targetSplit(noise4))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.multiBlobs(noise4))\n",
    "\n",
    "noise3 = .3\n",
    "\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.xorSplit(noise3))\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.targetSplit(noise3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can easily visualize the rendered datasets because they are generated in two dimensions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see what dfTrain, a simulated data that we just made, looks like"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfTrain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, dfTrain has two columns, vect and category."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotter(dfTrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a Machine Learning algorithm\n",
    "\n",
    "We can now pick a model, there are many more options in `scikit-learn`. These are just a few examples, which array along the machine learning \"tribes\" described in Pedro Domingos _The Master Algorithm_.\n",
    "\n",
    "Uncomment (remove the # in front of) each algorithm one at a time, then run the cell and subsequent cells to evaluate how it learns to understand the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bayes\n",
    "#clf = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "#Analogizes\n",
    "#clf = sklearn.svm.SVC(kernel = 'linear', probability = True) #slow, set probability = False to speed up\n",
    "#clf = sklearn.svm.SVC(kernel = 'poly', degree = 3, probability = True) #slower\n",
    "#clf = sklearn.neighbors.KNeighborsClassifier(5, weights='distance')# k, 'distance' or 'uniform'\n",
    "\n",
    "#Classical Regression\n",
    "#clf = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "#Symbolists\n",
    "#clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#clf = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "#Connectionists\n",
    "clf = sklearn.neural_network.MLPClassifier()\n",
    "\n",
    "#Ensemble\n",
    "#clf = sklearn.ensemble.GradientBoostingClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're using sklearn here. What is sklearn? It's an open source machine learning library for Python. We use sklearn because it features various machine learning algorithms and it works well with NumPy library. You don't need to understand the sklearn package in detail for the moment, but let's see what methods and attributes sklearn has, because we're going to use some of them:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dir(sklearn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we fit the model by giving it our training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf.fit(np.stack(dfTrain['vect'], axis=0), dfTrain['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, \"stack\" function is used. Why did we use stack function here? This is because dfTrain['vect'] is a sequence, while clf.fit() takes an array element. Let's see what dfTrain['vect'] looks like:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfTrain['vect']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's see what np.stack(dfTrain['vect']) does:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.stack(dfTrain['vect'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, stack function takes a sequence of arrays (which have the same shape) and joins them along a new axis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm evaluation\n",
    "\n",
    "We can look at few measurements of each classifier's performance by using the testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.evaluateClassifier(clf, dfTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This lets us look at which classes do better:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(clf, dfTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The greater the area under the curve the better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotMultiROC(clf, dfTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also look at the regions the classifer identifies as one class or the other:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotregions(clf, dfTrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now we do the same for real data\n",
    "\n",
    "Available data sets include:\n",
    "+ Reddit threads \"classified\" by thread topic\n",
    "+ 20 newsgroups \"classified\" by group topic\n",
    "+ Senate press releases \"classified\" by Senator (2 senators)\n",
    "+ Senate press releases \"classified\" by Senator (5 senators)\n",
    "+ Emails classified as Spam or Ham"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.loadReddit())\n",
    "dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.loadNewsGroups())\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.loadSenateSmall())\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.loadSenateLarge())\n",
    "#dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.loadSpam())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bayes\n",
    "#clf = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "#Analogizes\n",
    "#clf = sklearn.svm.SVC(kernel = 'linear', probability = True) #slow, set probability = False to speed up, but lose ROC\n",
    "#clf = sklearn.svm.SVC(kernel = 'poly', degree = 3, probability = True) #slower\n",
    "#clf = sklearn.neighbors.KNeighborsClassifier(5, weights='distance')# k, 'distance' or 'uniform'\n",
    "\n",
    "#Classical Regression\n",
    "#clf = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "#Symbolists\n",
    "#clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#clf = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "#Connectionists\n",
    "clf = sklearn.neural_network.MLPClassifier()\n",
    "\n",
    "#Ensemble\n",
    "#clf = sklearn.ensemble.GradientBoostingClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf.fit(np.stack(dfTrain['vect'], axis=0), dfTrain['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.evaluateClassifier(clf, dfTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(clf, dfTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotMultiROC(clf, dfTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotregions(clf, dfTrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Go back through all of the cells above and generate 10 distinct artificial datasets and classify them with all of the available methods. Add a cell immediately below and describe which classifier(s) worked best with which artificially constructed data source and why. Then go through all of the empirical datasets (i.e., Newsgroups, Senate Small, Senate Large, Email Spam) and classify them with all available methods. Add a second cell immediately below and describe which classifier(s) worked best with which data set and why.\n",
    "\n",
    "<span style=\"color:red\">***Stretch*** (but also required) Wander through the SKLearn documentation available [here](http://scikit-learn.org/stable/), particularly perusing the classifiers. In cells following, identify and implement a new classifier that we have not yet used (e.g., AdaBoost, CART) on one artificial dataset and one real dataset (used above). Then, in the next cell describe the classifier, detail how it compares with the approaches above, and why it performed better or worse than others."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10 Artificial Datasets\n",
    "\n",
    "1) Linearly divided data\n",
    "\n",
    "- Linear SVM, Naive Bayes, Logistic regression and NN do well in capturing the linear boundary - due to their simple linear model formulations\n",
    "- Poly kernel SVM performs slightly worse, does not draw linear boundary due to the non linear kernel\n",
    "- KNN, decision trees, and boosting overfit the data and fail to capture the linear relationship correctly. These models are overly complex for the task and are prone to overfitting.\n",
    "\n",
    "2) XOR split\n",
    "\n",
    "- NN, Decision trees, random forest, boosting work best - NN superior. These models can capture the opposing relationship.\n",
    "- KNN performance worse, but better than the models below. Captures nonlinearity due to non parametric form\n",
    "- Naive Bayes, logistic regression, linear SVM perform very poorly - they cannot capture purely non linear relationships\n",
    "\n",
    "3) Target split\n",
    "\n",
    "- Similar results to XOR split. Poly kernel SVM not able to capture the relationship\n",
    "\n",
    "4) Blobs\n",
    "\n",
    "- NN, Naive Bayes, SVM, Trees, Ensemble, Forests, Logistic regression do well to capture the blob shapes (somewhat)\n",
    "- KNN performs worse than above models\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Real Datasets\n",
    "\n",
    "Reddit Dataset\n",
    "\n",
    "- Naive Bayes reasonably accurate at classifying data.\n",
    "- SVM, NN, Ensemble performs very well and is a high performance classifier on many metrics.\n",
    "- KNN performs reasonably well but worse than SVM - tends to draw arbitrary boundaries between close classes\n",
    "- Logistic regression performs the best amongst all\n",
    "\n",
    "Newsroom Corpus\n",
    "\n",
    "- Logistic regression, NN, Ensemble perform moderately well\n",
    "- SVM performs worse than above models\n",
    "- KNN performs almost like random.\n",
    "\n",
    "Senate press release dataset\n",
    "\n",
    "- Almost all the models except KNN and SVM poly kernel perform exceedingly well, as the boundary appears to be linear\n",
    "- KNN overfits the data and consequently performs worst"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "dfTrain, dfTest = lucem_illud.trainTestSplit(lucem_illud.loadSenateSmall())\n",
    "\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "clf.fit(np.stack(dfTrain['vect'], axis=0), dfTrain['category'])\n",
    "\n",
    "\n",
    "lucem_illud_2020.evaluateClassifier(clf, dfTest)\n",
    "\n",
    "\n",
    "\n",
    "lucem_illud_2020.plotConfusionMatrix(clf, dfTest)\n",
    "\n",
    "\n",
    "\n",
    "lucem_illud_2020.plotMultiROC(clf, dfTest)\n",
    "\n",
    "\n",
    "lucem_illud_2020.plotregions(clf, dfTrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfTrain, dfTest = lucem_illud_2020.trainTestSplit(lucem_illud_2020.multiBlobs(noise))\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "clf.fit(np.stack(dfTrain['vect'], axis=0), dfTrain['category'])\n",
    "\n",
    "\n",
    "lucem_illud_2020.evaluateClassifier(clf, dfTest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(clf, dfTest)\n",
    "\n",
    "\n",
    "\n",
    "lucem_illud_2020.plotMultiROC(clf, dfTest)\n",
    "\n",
    "\n",
    "lucem_illud_2020.plotregions(clf, dfTrain)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fitting QDA from sklearn documentation\n",
    "\n",
    "- QDA fits gaussian distributions with distinct covariance matrices to the classes. Points with greatest prob of belonging to a class are labelled respectively.\n",
    "- As we see from the results below, QDA does not capture the relationship between the classes well of the Senate data.\n",
    "- Perhaps its linear cousin LDA would have performed better, as the data seems to not be guassian distributions and simplifying the model may yield mroe generalizable results.\n",
    "- QDA performs quite well with the Blob data - as good as the best models from the ones previosuly tried. This makes sense as the blobs are gaussian. Note the non linear boundaries - LDA produces linear only."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clinton / Obama Press Releases\n",
    "\n",
    "We often will not have nicely prepared data, so we will work though the proccess of cleaning and structuring in more detail here:\n",
    "\n",
    "While the Clinton and Obama Senatorial Press Releases are not hand-coded, we can imagine that we have been given a stack of such press releases, but lost the metadata associated with which senatorial office issued which. If we label a few of them, how well can our classifier do at recovering the rest?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ObamaClintonReleases = pandas.read_csv('../data/ObamaClintonReleases.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'm pretty sure that you're all familiar with pandas, but, just to clarify: why do we use pandas here? pandas is a Python library which is widely used for analyzing and wrangling data. In particular, pandas loads data and creates data frame, a Python object that looks familiar to us (since it looks like a excel table) and easy to work with. So, using pandas.read_csv function, we take in the csv file and convert it into a data frame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type(ObamaClintonReleases)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's a DataFrame! and it looks like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ObamaClintonReleases"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neat! Let's turn the 'targetSenator' column into a binary category variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ObamaClintonReleases['category'] = [s == 'Obama' for s in ObamaClintonReleases['targetSenator']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenize and normalize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ObamaClintonReleases['tokenized_text'] = ObamaClintonReleases['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "ObamaClintonReleases['normalized_text'] = ObamaClintonReleases['tokenized_text'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's split the data into training data and testing data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_data_df, test_data_df = lucem_illud_2020.trainTestSplit(ObamaClintonReleases, holdBackFraction=holdBackFraction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_data_df))\n",
    "print(len(test_data_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's try with a logistic regression, which may be familiar to you from statistical methods classes. First, we must turn the training dataset into a tf-idf matrix (`lucem_illud_2020.generateVecs()` will help with this but for now we are doing it the long way):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "TFVects = TFVectorizer.fit_transform(train_data_df['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The core function here is TfidfVectorizer, which takes a collection of raw documents and turn them to a tf-idf matrix. Just to recap: tf-idf means term frequency-inverse document frequency, a statistic (or, more precisely, a product of two statistics, term frequency and inverse document frequency) that shows the importance of a term vis-a-vis documents. TF, or, term frequency, counts how many times a term is used in a document; IDF, or, inverse-document-frequency, measures common or rare a term appears across documents.\n",
    "\n",
    "Let's look at three parameters of TfidfVectorizer: max_df = 100, min_df = 2, and norm='l2'. What do those parameters mean?\n",
    "\n",
    "(1) max_df = 100\n",
    "\n",
    "Here, we specified a thredhold of 100, and the terms that have a document frequency higher than 100 would be ignored.\n",
    "\n",
    "(2) min_df = 2\n",
    "\n",
    "We specified a lower bound, 2, and the terms that have a document frequency lower than 2 will be ignored.\n",
    "\n",
    "(3) norm = 'l2'\n",
    "\n",
    "This parameter is about vector normalization. In machine learning, we commonly normalize vectors, i.e., change the length of vectors and turn them into a unit vector, before passing them into algorithms. There are various ways of normalizations, and this parameter specifies how we normalize vectors. Here, we set the norm to l2, in which case, we normalize the vectors such that squares of vector elements sums to 1. Alternatively, we can set it to l1, in which case the sum of absolute values of vector elements, not the square of vector elements, is 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fit_transform() literally fits to data and then transform it. So, fit_transform() is just a combination of two steps--(1) fitting parameters to data; (2) then, using the vocabulary and document frequencies learned by fit(), transforming documents into document-term matrix. So, it's the same as fit followed by transform."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we can use the CountVectorizer instead, which simply produces a matrix of word counts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TFVects.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can save this in the dataframe to make things easier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_df['vect'] = [np.array(v).flatten() for v in TFVects.todense()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks simple, but we need to know what todense() does here. todense() returns a dense matrix representation of the matrix. Why do we need this? As you can see above, the TFVects, a document-term matrix, has 11349 columns, and this matrix is sparse, in the sense that it is comprised mostly of zeros. Dense matrices, in contrast, are the matrices that are comprised of mostly non-zeros. Then why do we make sparse matrices into dense ones? Because zero values don't contain important information but take up so much memory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a regression, we cannot have more variables than cases. So, we need to first do a dimension reduction. First, we will approah this with PCA. You have previously seen this in week 3. Here we are not concerned about visualization, but rather classification and so all principal components are calculated. Watch out: we have to use `stack` not `sum` for combining the vectors. We note that you could also use topic loading and embedding dimensions as featured variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA()\n",
    "reduced_data = pca.fit_transform(np.stack(train_data_df['vect'], axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can store the PCA space vectors in the dataframe too:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_df['pca'] = [r for r in reduced_data]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualization in 2D:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.axis('off')\n",
    "pallet = seaborn.color_palette(palette='coolwarm', n_colors = 2)\n",
    "\n",
    "#Plot Obama\n",
    "a = np.stack(train_data_df[train_data_df['category']]['pca'])\n",
    "ax.scatter(a[:,0], a[:, 1], c = pallet[0], label = \"True\")\n",
    "\n",
    "#Plot not Obama\n",
    "a = np.stack(train_data_df[train_data_df['category'].eq(False)]['pca'])\n",
    "ax.scatter(a[:,0], a[:, 1], c = pallet[1], label = \"False\")\n",
    "\n",
    "ax.legend(loc = 'upper right', title = 'Is Obama')\n",
    "plt.title('True Classes, Training Set')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PCA cannot distinguish Obama very well. Let's perform a screeplot to see how many Principal Components we need."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = len(train_data_df)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (16, 5), sharey=True)\n",
    "\n",
    "eigen_vals = np.arange(n) + 1\n",
    "ax1.plot(eigen_vals, pca.explained_variance_ratio_, 'ro-', linewidth=1)\n",
    "ax1.set_title('Scree Plot (Full)')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "eigen_vals = np.arange(50) + 1\n",
    "ax2.plot(eigen_vals, pca.explained_variance_ratio_[:50], 'ro-', linewidth=1)\n",
    "ax2.set_title('Scree Plot (First 50 Principal Components)')\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "\n",
    "eigen_vals = np.arange(20) + 1\n",
    "ax3.plot(eigen_vals, pca.explained_variance_ratio_[:20], 'ro-', linewidth=2)\n",
    "ax3.set_title('Scree Plot (First 50 Principal Components)')\n",
    "ax3.set_xlabel('Principal Component')\n",
    "ax3.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's choose the first 10 pricipal components as our covariates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_10'] = train_data_df['pca'].apply(lambda x: x[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we fit a logistic regression to our data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression()\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_10'], axis=0), train_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how the logistic regression performs on the training dataset from which we develop the model. Unfortunately, the mean accuracy is only about 64%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logistic.score(np.stack(train_data_df['pca_reduced_10'], axis=0), train_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How does it perform on the testing dataset, which we \"held out\" and did not use for model training? We need to repeat all the steps on the testing data, but without retraining:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create vectors\n",
    "TFVects_test = TFVectorizer.transform(test_data_df['text'])\n",
    "test_data_df['vect'] = [np.array(v).flatten() for v in TFVects_test.todense()]\n",
    "\n",
    "#PCA\n",
    "reduced_data_test = pca.transform(np.stack(test_data_df['vect'], axis=0))\n",
    "test_data_df['pca'] = [r for r in reduced_data_test]\n",
    "test_data_df['pca_reduced_10'] = test_data_df['pca'].apply(lambda x: x[:10])\n",
    "\n",
    "#Test\n",
    "logistic.score(np.stack(test_data_df['pca_reduced_10'], axis=0), test_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Slightly poorer. How about using more dimensions (40)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_40'] = train_data_df['pca'].apply(lambda x: x[:40])\n",
    "test_data_df['pca_reduced_40'] = test_data_df['pca'].apply(lambda x: x[:40])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_40'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_40'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_40'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or still more (100)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_100'] = train_data_df['pca'].apply(lambda x: x[:100])\n",
    "test_data_df['pca_reduced_100'] = test_data_df['pca'].apply(lambda x: x[:100])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_100'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_100'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_100'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or even more (200)!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_200'] = train_data_df['pca'].apply(lambda x: x[:200])\n",
    "test_data_df['pca_reduced_200'] = test_data_df['pca'].apply(lambda x: x[:200])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_200'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_200'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_200'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is becoming ridiculous (400)!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_400'] = train_data_df['pca'].apply(lambda x: x[:400])\n",
    "test_data_df['pca_reduced_400'] = test_data_df['pca'].apply(lambda x: x[:400])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_400'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_400'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_400'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Increasing the number of covariates would overfit our data, and it seems that using a logistic regression, our prediction accuracy is at best about 65%. We can, however, try a logistic regression that uses the TF-IDF scores for each word, but with an L1 regularization or L1-norm loss function, which is also known as least absolute deviations (LAD), least absolute errors (LAE) or L1 penalty. It minimizes the sum of the absolute differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$) and prunes all insignificant variables (i.e., word TF-IDF scores):\n",
    "\n",
    "$S=\\sum^n_{i=1}|y_i=f(x_i)|$\n",
    "\n",
    "The result is a model retaining only the most individually significant features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logistic_l1= sklearn.linear_model.LogisticRegression(penalty='l2')\n",
    "logistic_l1.fit(np.stack(train_data_df['vect'], axis=0), train_data_df['category'])\n",
    "print(logistic_l1.score(np.stack(train_data_df['vect'], axis=0), train_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model using training data, and then test it on the testing data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(logistic_l1.score(np.stack(test_data_df['vect'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "81% accuracy seems like the best we can get by using a logistic regression.\n",
    "\n",
    "Now let's try with Naive Bayes. Classically, it is trained with word counts, but TF-IDF vectors are also quite good:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "naiveBayes = sklearn.naive_bayes.BernoulliNB()\n",
    "naiveBayes.fit(np.stack(train_data_df['vect'], axis=0), train_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "print(naiveBayes.score(np.stack(train_data_df['vect'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(naiveBayes.score(np.stack(test_data_df['vect'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A bit better than the logit, but that's just looking at the accuracy. What about other measures? Let's first save the predictions in the dataframe to save use rerunning the model every time:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data_df['nb_predict'] = naiveBayes.predict(np.stack(test_data_df['vect'], axis=0))\n",
    "test_data_df['nb_predict_prob_true'] = naiveBayes.predict_proba(np.stack(test_data_df['vect'], axis=0))[:,0] #other is prop false"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Precision:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn.metrics.precision_score(test_data_df['category'], test_data_df['nb_predict'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn.metrics.recall_score(test_data_df['category'], test_data_df['nb_predict'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "F1-measure:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(test_data_df['category'], test_data_df['nb_predict'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at how well our posterior distribution looks relative to the truth."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(False)\n",
    "ax.set_frame_on(False)\n",
    "test_data_df[test_data_df['category'].eq(True)]['nb_predict_prob_true'].hist(alpha = 0.5, ax = ax, bins = 10, label = 'True', color = 'red')\n",
    "test_data_df[test_data_df['category'].eq(False)]['nb_predict_prob_true'].hist(alpha = 0.5, ax = ax, bins = 10, label = 'False', color = 'blue')\n",
    "ax.set_xlim((0,1.1))\n",
    "ax.legend(title = \"Is Obama\")\n",
    "ax.set_xlabel('posterior')\n",
    "ax.set_ylabel('counts')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The classification is suprisingly accurate. We can even look at what words are most influential with a bit of simple math:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Top indices\n",
    "trueVals, falseVals = naiveBayes.feature_log_prob_\n",
    "\n",
    "words_dict = {\n",
    "    'Obama' : [],\n",
    "    'Obama_log_prob' : [],\n",
    "    'Clinton' : [],\n",
    "    'Clinton_log_prob' : [],\n",
    "}\n",
    "\n",
    "for i, prob in sorted(enumerate(trueVals), key = lambda x:x[1], reverse=True)[:15]:\n",
    "    words_dict['Obama'].append(TFVectorizer.get_feature_names()[i])\n",
    "    words_dict['Obama_log_prob'].append(prob)\n",
    "\n",
    "for i, prob in sorted(enumerate(falseVals), key = lambda x:x[1], reverse=True)[:15]:\n",
    "    words_dict['Clinton'].append(TFVectorizer.get_feature_names()[i])\n",
    "    words_dict['Clinton_log_prob'].append(prob)\n",
    "\n",
    "pandas.DataFrame(words_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multinomial Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What if we want to classify our text into one of *many* classes? The multinomial Naive Bayes generating model assumes that document features (e.g., words) are generated by draws from a multinomial distribution (recall this gives the probability to observe a particular pattern of counts across features).\n",
    "\n",
    "Let's use again the dataset we used in week 3, the 20 newsgroup dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newsgroups = sklearn.datasets.fetch_20newsgroups(data_home = '../data') #Free data to play with: documents from a newsgroup corpus.\n",
    "newsgroups.target_names #Possible categories, i.e., the newsgroups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can pick specific categories, and pull the relevant training and testing sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics'] #Can change these of course\n",
    "\n",
    "newsgroupsDF = pandas.DataFrame(columns = ['text', 'category', 'source_file'])\n",
    "for category in target_categories:\n",
    "    print(\"Loading data for: {}\".format(category))\n",
    "    ng = sklearn.datasets.fetch_20newsgroups(categories = [category], remove=['headers', 'footers', 'quotes'], data_home = '../data')\n",
    "    newsgroupsDF = newsgroupsDF.append(pandas.DataFrame({'text' : ng.data, 'category' : [category] * len(ng.data), 'source_file' : ng.filenames}), ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to tokenize, and make a training and testing set:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newsgroupsDF['tokenized_text'] = newsgroupsDF['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "newsgroupsDF['normalized_text'] = newsgroupsDF['tokenized_text'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_ng_df, test_ng_df = lucem_illud_2020.trainTestSplit(newsgroupsDF, holdBackFraction=holdBackFraction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_ng_df))\n",
    "print(len(test_ng_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to extract features from the text. We can use built-in feature extraction to do so. We will use a tf-idf vectorizer, which converts the document into a vector of words with tf-idf weights (term-frequency inverse-document frequency). This gives high weight to words that show up a lot in a given document, but rarely across documents in the corpus (more distinctive)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TFVectorizer_ng = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "TFVects_ng = TFVectorizer_ng.fit_transform(train_ng_df['text'])\n",
    "train_ng_df['vect'] = [np.array(v).flatten() for v in TFVects_ng.todense()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can train the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MultinomialNB_ng = sklearn.naive_bayes.MultinomialNB()\n",
    "MultinomialNB_ng.fit(np.stack(train_ng_df['vect'], axis = 0), train_ng_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "...and save predictions to the dataframe:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ng_df['nb_predict'] = MultinomialNB_ng.predict(np.stack(train_ng_df['vect'], axis=0))\n",
    "print(\"Training score:\")\n",
    "print(MultinomialNB_ng.score(np.stack(train_ng_df['vect'], axis=0), train_ng_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ng_df[['category', 'nb_predict']][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks pretty good, lets examine the testing dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create vectors\n",
    "TFVects_test = TFVectorizer_ng.transform(test_ng_df['text'])\n",
    "test_ng_df['vect'] = [np.array(v).flatten() for v in TFVects_test.todense()]\n",
    "\n",
    "#Add to df\n",
    "test_ng_df['nb_predict'] = MultinomialNB_ng.predict(np.stack(test_ng_df['vect'], axis=0))\n",
    "\n",
    "#Test\n",
    "print(\"Testing score:\")\n",
    "print(MultinomialNB_ng.score(np.stack(test_ng_df['vect'], axis=0), test_ng_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can even use a confusion matrix, like we used last week for evaluating human coders relative to one another. Now we are evaluating our classifier relative to human coding. We'll just use the one in `lucem_illud_2020`, which requres a classifier and a dataframe with `'vect'` and `'category'` columns, like we have in the examples:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(MultinomialNB_ng, test_ng_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's calculate the precision, recall, and F-measures."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sklearn.metrics.precision_score(test_ng_df['category'], test_ng_df['nb_predict'], average = 'weighted')) #precision\n",
    "print(sklearn.metrics.recall_score(test_ng_df['category'], test_ng_df['nb_predict'], average = 'weighted')) #recall\n",
    "print(sklearn.metrics.f1_score(test_ng_df['category'], test_ng_df['nb_predict'], average = 'weighted')) #F-1 measure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can evaluate these per catagory. This has the same requiments as `plotConfusionMatrix`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.metrics.evaluateClassifier(MultinomialNB_ng, test_ng_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also plot the ROC curves. This has the same requiments as `plotConfusionMatrix`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotMultiROC(MultinomialNB_ng, test_ng_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we can plot the PCA space visualization:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotregions(MultinomialNB_ng, test_ng_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform Logistic and Na√Øve Bayes classification (binary or multinomial) using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project (e.g., these could be crowd-sourced codes gathered through Amazon Mechanical Turk last week). Visualize the confusion matrix for training and testing sets. Calculate precision, recall, the F-measure, and AUC, then perform an ROC visualization. How do these classifiers perform? Exrapolate codes from these models to all uncoded data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Twitter Personality Dataset\n",
    "- Contains the liked tweets of Twitter profiles who have self-labelled personality type codes in their biographies.\n",
    "- Will be using a subset (tweets from about 200 users) of the total dataset to explore whether we can classify users as low or high 'openness' (a personality trait) - coded by S/N (0/1) respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "N_df = pandas.read_csv('INTJ_tweets.csv')\n",
    "S_df = pandas.read_csv('ISFJ_tweets.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "N_df = N_df.sample(n = 3000)\n",
    "N_df['Openness'] = 1\n",
    "S_df = S_df.sample(n = 3000)\n",
    "S_df['Openness'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "N_df.drop('Unnamed: 0', axis = 1, inplace= True)\n",
    "S_df.drop('Unnamed: 0', axis = 1, inplace= True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "df = pandas.concat([N_df,S_df])\n",
    "print(len(df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "df['Liked text'] = df['Liked text'].apply(lambda x: x[16:])\n",
    "df.columns = ['text', 'category']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  category\n",
      "13422  \"@neilhaw10633832 @BreezerGalway @jneill @Mich...         1\n",
      "17048  \"@PaigeSully88 It's how we move forward which ...         1\n",
      "18402  \"The fog at 4am this morning was so beautiful,...         1\n",
      "3190   \"Day one of  #MakeOverYourMornings, getting a ...         1\n",
      "12640  \"Close up of Pluto from the New Horizons space...         1\n",
      "...                                                  ...       ...\n",
      "12043  \"Israel comes to a standstill with a two-minut...         0\n",
      "4248   \"Look at these funky little dudes go. Snails a...         0\n",
      "744    \"Lockdown lunch is back on the blog! \\ud83c\\ud...         0\n",
      "15529  \"Let's all take a moment to enjoy this. \\nhttp...         0\n",
      "6765   \"@PokkiDots @TheBlackLayers In case anyone wan...         0\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index                                               text  category  \\\n",
      "0     13422  \"@neilhaw10633832 @BreezerGalway @jneill @Mich...         1   \n",
      "1     17048  \"@PaigeSully88 It's how we move forward which ...         1   \n",
      "2     18402  \"The fog at 4am this morning was so beautiful,...         1   \n",
      "3      3190  \"Day one of  #MakeOverYourMornings, getting a ...         1   \n",
      "4     12640  \"Close up of Pluto from the New Horizons space...         1   \n",
      "...     ...                                                ...       ...   \n",
      "5995  12043  \"Israel comes to a standstill with a two-minut...         0   \n",
      "5996   4248  \"Look at these funky little dudes go. Snails a...         0   \n",
      "5997    744  \"Lockdown lunch is back on the blog! \\ud83c\\ud...         0   \n",
      "5998  15529  \"Let's all take a moment to enjoy this. \\nhttp...         0   \n",
      "5999   6765  \"@PokkiDots @TheBlackLayers In case anyone wan...         0   \n",
      "\n",
      "                                         tokenized_text  \\\n",
      "0     [@neilhaw10633832, @BreezerGalway, @jneill, @M...   \n",
      "1     [@PaigeSully88, It, 's, how, we, move, forward...   \n",
      "2     [The, fog, at, 4, am, this, morning, was, so, ...   \n",
      "3     [Day, one, of, MakeOverYourMornings, getting, ...   \n",
      "4     [Close, up, of, Pluto, from, the, New, Horizon...   \n",
      "...                                                 ...   \n",
      "5995  [Israel, comes, to, a, standstill, with, a, tw...   \n",
      "5996  [Look, at, these, funky, little, dudes, go, Sn...   \n",
      "5997  [Lockdown, lunch, is, back, on, the, blog, \\ud...   \n",
      "5998  [Let, 's, all, take, a, moment, to, enjoy, thi...   \n",
      "5999  [@PokkiDots, @TheBlackLayers, In, case, anyone...   \n",
      "\n",
      "                                        normalized_text  \n",
      "0     [@neilhaw10633832, @breezergalway, @jneill, @m...  \n",
      "1                   [@paigesully88, forward, important]  \n",
      "2     [fog, morning, beautiful, sit, cup, tea, watch...  \n",
      "3     [day, makeoveryourmornings, get, great, start,...  \n",
      "4     [close, pluto, new, horizon, space, probe, htt...  \n",
      "...                                                 ...  \n",
      "5995  [israel, come, standstill, minute, siren, wail...  \n",
      "5996  [look, funky, little, dude, snail, tiny, camer...  \n",
      "5997  [lockdown, lunch, blog, \\ud83c\\udf7d\\n\\nmy, la...  \n",
      "5998  [let, moment, enjoy, \\nhttps://t.co, mx7goklj4...  \n",
      "5999  [@pokkidots, @theblacklayers, case, want, ton,...  \n",
      "\n",
      "[6000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df['tokenized_text'] = df['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "df['normalized_text'] = df['tokenized_text'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))\n",
    "\n",
    "df.reset_index(inplace = True)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's split the data into training data and testing data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_data_df, test_data_df = lucem_illud_2020.trainTestSplit(df, holdBackFraction=holdBackFraction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_df))\n",
    "print(len(test_data_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's try with a logistic regression, which may be familiar to you from statistical methods classes. First, we must turn the training dataset into a tf-idf matrix (`lucem_illud_2020.generateVecs()` will help with this but for now we are doing it the long way):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "TFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "TFVects = TFVectorizer.fit_transform(train_data_df['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The core function here is TfidfVectorizer, which takes a collection of raw documents and turn them to a tf-idf matrix. Just to recap: tf-idf means term frequency-inverse document frequency, a statistic (or, more precisely, a product of two statistics, term frequency and inverse document frequency) that shows the importance of a term vis-a-vis documents. TF, or, term frequency, counts how many times a term is used in a document; IDF, or, inverse-document-frequency, measures common or rare a term appears across documents.\n",
    "\n",
    "Let's look at three parameters of TfidfVectorizer: max_df = 100, min_df = 2, and norm='l2'. What do those parameters mean?\n",
    "\n",
    "(1) max_df = 100\n",
    "\n",
    "Here, we specified a thredhold of 100, and the terms that have a document frequency higher than 100 would be ignored.\n",
    "\n",
    "(2) min_df = 2\n",
    "\n",
    "We specified a lower bound, 2, and the terms that have a document frequency lower than 2 will be ignored.\n",
    "\n",
    "(3) norm = 'l2'\n",
    "\n",
    "This parameter is about vector normalization. In machine learning, we commonly normalize vectors, i.e., change the length of vectors and turn them into a unit vector, before passing them into algorithms. There are various ways of normalizations, and this parameter specifies how we normalize vectors. Here, we set the norm to l2, in which case, we normalize the vectors such that squares of vector elements sums to 1. Alternatively, we can set it to l1, in which case the sum of absolute values of vector elements, not the square of vector elements, is 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fit_transform() literally fits to data and then transform it. So, fit_transform() is just a combination of two steps--(1) fitting parameters to data; (2) then, using the vocabulary and document frequencies learned by fit(), transforming documents into document-term matrix. So, it's the same as fit followed by transform."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we can use the CountVectorizer instead, which simply produces a matrix of word counts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "data": {
      "text/plain": "(4800, 6075)"
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFVects.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can save this in the dataframe to make things easier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "train_data_df['vect'] = [np.array(v).flatten() for v in TFVects.todense()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks simple, but we need to know what todense() does here. todense() returns a dense matrix representation of the matrix. Why do we need this? As you can see above, the TFVects, a document-term matrix, has 11349 columns, and this matrix is sparse, in the sense that it is comprised mostly of zeros. Dense matrices, in contrast, are the matrices that are comprised of mostly non-zeros. Then why do we make sparse matrices into dense ones? Because zero values don't contain important information but take up so much memory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a regression, we cannot have more variables than cases. So, we need to first do a dimension reduction. First, we will approah this with PCA. You have previously seen this in week 3. Here we are not concerned about visualization, but rather classification and so all principal components are calculated. Watch out: we have to use `stack` not `sum` for combining the vectors. We note that you could also use topic loading and embedding dimensions as featured variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA()\n",
    "reduced_data = pca.fit_transform(np.stack(train_data_df['vect'], axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can store the PCA space vectors in the dataframe too:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "train_data_df['pca'] = [r for r in reduced_data]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualization in 2D:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\\n            ...\\n            1, 1, 1, 0, 0, 0, 1, 0, 0, 1],\\n           dtype='int64', length=4800)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-184-5837d5869ef9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m#Plot Obama\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0ma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtrain_data_df\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'category'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'pca'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[0max\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscatter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpallet\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"True\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\partha\\pycharmprojects\\companalysis\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3028\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3029\u001B[0m                 \u001B[0mkey\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3030\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_listlike_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mraise_missing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3031\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3032\u001B[0m         \u001B[1;31m# take() does not accept boolean indexers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\partha\\pycharmprojects\\companalysis\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_get_listlike_indexer\u001B[1;34m(self, key, axis, raise_missing)\u001B[0m\n\u001B[0;32m   1263\u001B[0m             \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_indexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0max\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reindex_non_unique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1264\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1265\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_validate_read_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mraise_missing\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mraise_missing\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1266\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mkeyarr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1267\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\partha\\pycharmprojects\\companalysis\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m_validate_read_indexer\u001B[1;34m(self, key, indexer, axis, raise_missing)\u001B[0m\n\u001B[0;32m   1305\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mmissing\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1306\u001B[0m                 \u001B[0maxis_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_axis_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1307\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1309\u001B[0m             \u001B[0max\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_axis\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"None of [Int64Index([1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\\n            ...\\n            1, 1, 1, 0, 0, 0, 1, 0, 0, 1],\\n           dtype='int64', length=4800)] are in the [columns]\""
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.axis('off')\n",
    "pallet = seaborn.color_palette(palette='coolwarm', n_colors = 2)\n",
    "\n",
    "#Plot Obama\n",
    "a = np.stack(train_data_df[train_data_df['category']]['pca'])\n",
    "ax.scatter(a[:,0], a[:, 1], c = pallet[0], label = \"True\")\n",
    "\n",
    "#Plot not Obama\n",
    "a = np.stack(train_data_df[train_data_df['category'].eq(False)]['pca'])\n",
    "ax.scatter(a[:,0], a[:, 1], c = pallet[1], label = \"False\")\n",
    "\n",
    "ax.legend(loc = 'upper right', title = 'Is Obama')\n",
    "plt.title('True Classes, Training Set')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PCA cannot distinguish Obama very well. Let's perform a screeplot to see how many Principal Components we need."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1152x360 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(train_data_df)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize = (16, 5), sharey=True)\n",
    "\n",
    "eigen_vals = np.arange(n) + 1\n",
    "ax1.plot(eigen_vals, pca.explained_variance_ratio_, 'ro-', linewidth=1)\n",
    "ax1.set_title('Scree Plot (Full)')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "eigen_vals = np.arange(50) + 1\n",
    "ax2.plot(eigen_vals, pca.explained_variance_ratio_[:50], 'ro-', linewidth=1)\n",
    "ax2.set_title('Scree Plot (First 50 Principal Components)')\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "\n",
    "eigen_vals = np.arange(20) + 1\n",
    "ax3.plot(eigen_vals, pca.explained_variance_ratio_[:20], 'ro-', linewidth=2)\n",
    "ax3.set_title('Scree Plot (First 50 Principal Components)')\n",
    "ax3.set_xlabel('Principal Component')\n",
    "ax3.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's choose the first 10 pricipal components as our covariates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "train_data_df['pca_reduced_10'] = train_data_df['pca'].apply(lambda x: x[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we fit a logistic regression to our data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression()\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_10'], axis=0), train_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how the logistic regression performs on the training dataset from which we develop the model. Unfortunately, the mean accuracy is only about 64%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "0.530625"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(np.stack(train_data_df['pca_reduced_10'], axis=0), train_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How does it perform on the testing dataset, which we \"held out\" and did not use for model training? We need to repeat all the steps on the testing data, but without retraining:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5416666666666666"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create vectors\n",
    "TFVects_test = TFVectorizer.transform(test_data_df['text'])\n",
    "test_data_df['vect'] = [np.array(v).flatten() for v in TFVects_test.todense()]\n",
    "\n",
    "#PCA\n",
    "reduced_data_test = pca.transform(np.stack(test_data_df['vect'], axis=0))\n",
    "test_data_df['pca'] = [r for r in reduced_data_test]\n",
    "test_data_df['pca_reduced_10'] = test_data_df['pca'].apply(lambda x: x[:10])\n",
    "\n",
    "#Test\n",
    "logistic.score(np.stack(test_data_df['pca_reduced_10'], axis=0), test_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Slightly poorer. How about using more dimensions (40)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.57375\n",
      "Testing:\n",
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "train_data_df['pca_reduced_40'] = train_data_df['pca'].apply(lambda x: x[:40])\n",
    "test_data_df['pca_reduced_40'] = test_data_df['pca'].apply(lambda x: x[:40])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_40'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_40'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_40'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or still more (100)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.599375\n",
      "Testing:\n",
      "0.5466666666666666\n"
     ]
    }
   ],
   "source": [
    "train_data_df['pca_reduced_100'] = train_data_df['pca'].apply(lambda x: x[:100])\n",
    "test_data_df['pca_reduced_100'] = test_data_df['pca'].apply(lambda x: x[:100])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_100'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_100'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_100'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or even more (200)!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.6577083333333333\n",
      "Testing:\n",
      "0.5766666666666667\n"
     ]
    }
   ],
   "source": [
    "train_data_df['pca_reduced_200'] = train_data_df['pca'].apply(lambda x: x[:200])\n",
    "test_data_df['pca_reduced_200'] = test_data_df['pca'].apply(lambda x: x[:200])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_200'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_200'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_200'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is becoming ridiculous (400)!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.6964583333333333\n",
      "Testing:\n",
      "0.5925\n"
     ]
    }
   ],
   "source": [
    "train_data_df['pca_reduced_400'] = train_data_df['pca'].apply(lambda x: x[:400])\n",
    "test_data_df['pca_reduced_400'] = test_data_df['pca'].apply(lambda x: x[:400])\n",
    "\n",
    "logistic.fit(np.stack(train_data_df['pca_reduced_400'], axis=0), train_data_df['category'])\n",
    "\n",
    "print(\"Training:\")\n",
    "print(logistic.score(np.stack(train_data_df['pca_reduced_400'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(logistic.score(np.stack(test_data_df['pca_reduced_400'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Increasing the number of covariates would overfit our data, and it seems that using a logistic regression, our prediction accuracy is at best about 65%. We can, however, try a logistic regression that uses the TF-IDF scores for each word, but with an L1 regularization or L1-norm loss function, which is also known as least absolute deviations (LAD), least absolute errors (LAE) or L1 penalty. It minimizes the sum of the absolute differences (S) between the target value ($Y_i$) and the estimated values ($f(x_i)$) and prunes all insignificant variables (i.e., word TF-IDF scores):\n",
    "\n",
    "$S=\\sum^n_{i=1}|y_i=f(x_i)|$\n",
    "\n",
    "The result is a model retaining only the most individually significant features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89875\n"
     ]
    }
   ],
   "source": [
    "logistic_l1= sklearn.linear_model.LogisticRegression(penalty='l2')\n",
    "logistic_l1.fit(np.stack(train_data_df['vect'], axis=0), train_data_df['category'])\n",
    "print(logistic_l1.score(np.stack(train_data_df['vect'], axis=0), train_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model using training data, and then test it on the testing data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64\n"
     ]
    }
   ],
   "source": [
    "print(logistic_l1.score(np.stack(test_data_df['vect'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "81% accuracy seems like the best we can get by using a logistic regression.\n",
    "\n",
    "Now let's try with Naive Bayes. Classically, it is trained with word counts, but TF-IDF vectors are also quite good:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "BernoulliNB()"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naiveBayes = sklearn.naive_bayes.BernoulliNB()\n",
    "naiveBayes.fit(np.stack(train_data_df['vect'], axis=0), train_data_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.8695833333333334\n",
      "Testing:\n",
      "0.6766666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Training:\")\n",
    "print(naiveBayes.score(np.stack(train_data_df['vect'], axis=0), train_data_df['category']))\n",
    "print(\"Testing:\")\n",
    "print(naiveBayes.score(np.stack(test_data_df['vect'], axis=0), test_data_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A bit better than the logit, but that's just looking at the accuracy. What about other measures? Let's first save the predictions in the dataframe to save use rerunning the model every time:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "test_data_df['nb_predict'] = naiveBayes.predict(np.stack(test_data_df['vect'], axis=0))\n",
    "test_data_df['nb_predict_prob_true'] = naiveBayes.predict_proba(np.stack(test_data_df['vect'], axis=0))[:,0] #other is prop false"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Precision:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7390396659707724"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.precision_score(test_data_df['category'], test_data_df['nb_predict'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5737439222042139"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(test_data_df['category'], test_data_df['nb_predict'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "F1-measure:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "0.645985401459854"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.f1_score(test_data_df['category'], test_data_df['nb_predict'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at how well our posterior distribution looks relative to the truth."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(False)\n",
    "ax.set_frame_on(False)\n",
    "test_data_df[test_data_df['category'].eq(True)]['nb_predict_prob_true'].hist(alpha = 0.5, ax = ax, bins = 10, label = 'True', color = 'red')\n",
    "test_data_df[test_data_df['category'].eq(False)]['nb_predict_prob_true'].hist(alpha = 0.5, ax = ax, bins = 10, label = 'False', color = 'blue')\n",
    "ax.set_xlim((0,1.1))\n",
    "ax.legend(title = \"Has High Openness\")\n",
    "ax.set_xlabel('posterior')\n",
    "ax.set_ylabel('counts')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The classification is suprisingly accurate. We can even look at what words are most influential with a bit of simple math:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "   High Openness  O_log_prob Low Openness  L_log_prob\n0          happy   -3.765758         want   -3.716511\n1            got   -3.802125        years   -3.769621\n2         really   -3.820818        great   -3.806662\n3            did   -3.839866          way   -3.864931\n4        u2019re   -3.839866          did   -3.948313\n5           make   -3.859284       really   -3.970292\n6            let   -3.919908      twitter   -3.970292\n7          ude02   -3.940962         life   -4.015754\n8          u2764   -3.962468         make   -4.015754\n9            way   -3.984447       things   -4.015754\n10     president   -4.006920         week   -4.015754\n11          hope   -4.029909         best   -4.039285\n12          feel   -4.053440          got   -4.039285\n13         years   -4.053440       thanks   -4.039285\n14        better   -4.077537        world   -4.039285",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>High Openness</th>\n      <th>O_log_prob</th>\n      <th>Low Openness</th>\n      <th>L_log_prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>happy</td>\n      <td>-3.765758</td>\n      <td>want</td>\n      <td>-3.716511</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>got</td>\n      <td>-3.802125</td>\n      <td>years</td>\n      <td>-3.769621</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>really</td>\n      <td>-3.820818</td>\n      <td>great</td>\n      <td>-3.806662</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>did</td>\n      <td>-3.839866</td>\n      <td>way</td>\n      <td>-3.864931</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>u2019re</td>\n      <td>-3.839866</td>\n      <td>did</td>\n      <td>-3.948313</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>make</td>\n      <td>-3.859284</td>\n      <td>really</td>\n      <td>-3.970292</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>let</td>\n      <td>-3.919908</td>\n      <td>twitter</td>\n      <td>-3.970292</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ude02</td>\n      <td>-3.940962</td>\n      <td>life</td>\n      <td>-4.015754</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>u2764</td>\n      <td>-3.962468</td>\n      <td>make</td>\n      <td>-4.015754</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>way</td>\n      <td>-3.984447</td>\n      <td>things</td>\n      <td>-4.015754</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>president</td>\n      <td>-4.006920</td>\n      <td>week</td>\n      <td>-4.015754</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>hope</td>\n      <td>-4.029909</td>\n      <td>best</td>\n      <td>-4.039285</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>feel</td>\n      <td>-4.053440</td>\n      <td>got</td>\n      <td>-4.039285</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>years</td>\n      <td>-4.053440</td>\n      <td>thanks</td>\n      <td>-4.039285</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>better</td>\n      <td>-4.077537</td>\n      <td>world</td>\n      <td>-4.039285</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top indices\n",
    "trueVals, falseVals = naiveBayes.feature_log_prob_\n",
    "\n",
    "words_dict = {\n",
    "    'High Openness' : [],\n",
    "    'O_log_prob' : [],\n",
    "    'Low Openness' : [],\n",
    "    'L_log_prob' : [],\n",
    "}\n",
    "\n",
    "for i, prob in sorted(enumerate(trueVals), key = lambda x:x[1], reverse=True)[:15]:\n",
    "    words_dict['High Openness'].append(TFVectorizer.get_feature_names()[i])\n",
    "    words_dict['O_log_prob'].append(prob)\n",
    "\n",
    "for i, prob in sorted(enumerate(falseVals), key = lambda x:x[1], reverse=True)[:15]:\n",
    "    words_dict['Low Openness'].append(TFVectorizer.get_feature_names()[i])\n",
    "    words_dict['L_log_prob'].append(prob)\n",
    "\n",
    "pandas.DataFrame(words_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Trees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decision trees can be used to predict both categorical/class labels (i.e., classification) and continuous labels (i.e., regression)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blobs_df = lucem_illud_2020.multiBlobs(noise=.2, centers=[(0,0), (0,5), (5,0), (-5,0), (0,-5)])\n",
    "df_exampleTree_train, df_exampleTree_test = lucem_illud_2020.trainTestSplit(blobs_df)\n",
    "lucem_illud_2020.plotter(df_exampleTree_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we import our Decision Tree classifier from sklearn.tree (familiar syntax) and fit it using the fit method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_tree = sklearn.tree.DecisionTreeClassifier(max_depth=4,random_state=0)\n",
    "clf_tree.fit(np.stack(df_exampleTree_train['vect'], axis =0), df_exampleTree_train['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see what's going on visually with the classification:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotregions(clf_tree, df_exampleTree_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.evaluateClassifier(clf_tree, df_exampleTree_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets look at accuracy:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(df_exampleTree_test['category'],clf_tree.predict(np.stack(df_exampleTree_test['vect'], axis = 0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What happens if we trim the tree?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "depthvec = []\n",
    "scorevec = []\n",
    "for i in range(1,20):\n",
    "    tree2 = sklearn.tree.DecisionTreeClassifier(max_depth=i,random_state=0)\n",
    "    tree2.fit(np.stack(df_exampleTree_train['vect'], axis =0), df_exampleTree_train['category'])\n",
    "    score = sklearn.metrics.accuracy_score(df_exampleTree_test['category'], tree2.predict(np.stack(df_exampleTree_test['vect'], axis = 0)))\n",
    "    depthvec.append(i)\n",
    "    scorevec.append(score)\n",
    "plt.scatter(depthvec,scorevec)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can select different layers of the decision tree or \"prune\" it. At approximately four layers down in the decision tree, the shape is somewhat odd, suggesting that our model is overfitting beyond those four layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combining multiple overfitting estimators turns out to be a key idea in machine learning. This is called **bagging** and is a type of **ensemble** method. The idea is to make many randomized estimators--each can overfit, as decision trees are wont to do--but then to combine them, ultimately producing a better classification. A **random forest** is produced by bagging decision trees."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree = sklearn.tree.DecisionTreeClassifier(max_depth=10) #Create an instance of our decision tree classifier.\n",
    "\n",
    "bag = sklearn.ensemble.BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1) #Each tree uses up to 80% of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bag.fit(np.stack(df_exampleTree_train['vect'], axis =0), df_exampleTree_train['category']) #Fit the bagged classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotregions(bag, df_exampleTree_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.evaluateClassifier(bag, df_exampleTree_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(bag, df_exampleTree_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform decision tree and random forest classification (binary, multinomial or continuous) using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project. As with ***Exercise 2***, these could be crowd-sourced codes gathered through Amazon Mechanical Turk last week. Visualize the classification of data points. Calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). Now build an ensemble classifier by bagging trees into a random forest. Visualize the result. How do these classifiers perform? What does ensemble learning do?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K-Nearest Neighbors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The K-Nearest neighbors classifier takes a simpler premise than those before: Find the closest labeled datapoint in set and \"borrow\" its label.\n",
    "\n",
    "Let's use newsgroup data again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newsgroupsDF[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make a testing and training set:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_ng_df, test_ng_df = lucem_illud_2020.trainTestSplit(newsgroupsDF, holdBackFraction=holdBackFraction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's initialize our k-nearest neighbors classifier:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "weights=\"uniform\"\n",
    "clf_knearest = sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights=weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try to classify using the TF-IDF vectors:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TFVectorizer_ng = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "TFVects_ng = TFVectorizer_ng.fit_transform(train_ng_df['text'])\n",
    "train_ng_df['vect'] = [np.array(v).flatten() for v in TFVects_ng.todense()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_knearest.fit(np.stack(train_ng_df['vect'], axis = 0), train_ng_df['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.evaluateClassifier(clf_knearest, train_ng_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And lets look at the testing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create vectors\n",
    "TFVects_test = TFVectorizer_ng.transform(test_ng_df['text'])\n",
    "test_ng_df['vect'] = [np.array(v).flatten() for v in TFVects_test.todense()]\n",
    "\n",
    "#Add to df\n",
    "test_ng_df['nb_predict'] = clf_knearest.predict(np.stack(test_ng_df['vect'], axis=0))\n",
    "\n",
    "#Test\n",
    "print(\"Testing score:\")\n",
    "print(clf_knearest.score(np.stack(test_ng_df['vect'], axis=0), test_ng_df['category']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's produce another confusion matrix:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(clf_knearest, test_ng_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can produce the PCA space visual if you want, altough it can take a very long time, so we'll leave it optionally commented out:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#lucem_illud_2020.plotregions(clf_knearest, test_ng_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform and visualize k-nearest neighbor classification using training, testing and extrapolation (uncoded) data from texts and hand-classifications associated with your final project. Visualize the classification of data points and calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). Articulate how the *k*-nearest neighbor approach relates to *k*-means clustering explored in ***week 3***?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVMs\n",
    "\n",
    "Now we will examine Support Vector Machines, an approach that creates the partition that preserves the \"maximum margin\" between classes.\n",
    "\n",
    "We will use a few sub forums from reddit--which tend to share text rather than memes--namely `talesfromtechsupport`, `badroommates`, `weeabootales` and `relationships`. The top 100 text posts from each have been saved to `data/reddit.csv`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "redditDf = pandas.read_csv('../data/reddit.csv', index_col = 0)\n",
    "\n",
    "#Drop a couple missing values\n",
    "\n",
    "redditDf = redditDf.dropna()\n",
    "\n",
    "#Set category\n",
    "\n",
    "redditDf['category'] = redditDf['subreddit']\n",
    "\n",
    "#tokenize and normalize\n",
    "redditDf['tokenized_text'] = redditDf['text'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
    "redditDf['normalized_text'] = redditDf['tokenized_text'].apply(lambda x: lucem_illud_2020.normalizeTokens(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will tf.idf the data to make our vectors:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "redditTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, min_df=3, stop_words='english', norm='l2')\n",
    "redditTFVects = redditTFVectorizer.fit_transform([' '.join(l) for l in redditDf['normalized_text']])\n",
    "redditDf['vect'] = [np.array(v).flatten() for v in redditTFVects.todense()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initilize the model and make a train test split:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "holdBackFraction = .2\n",
    "train_redditDf, test_redditDf = lucem_illud_2020.trainTestSplit(redditDf, holdBackFraction=holdBackFraction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_svm = sklearn.svm.SVC(kernel='linear', probability = False)\n",
    "#probability = True is slower but  lets you call predict_proba()\n",
    "clf_svm.fit(np.stack(train_redditDf['vect'], axis=0), train_redditDf['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "...and consider the results:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.evaluateClassifier(clf_svm, test_redditDf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(clf_svm, test_redditDf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotregions(clf_svm, test_redditDf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Nets\n",
    "\n",
    "We include an example of a simple neural network, the Multi-layer Perceptron (MLP) that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. The following figure shows a one hidden layer MLP with scalar output. ![title](../data/multilayerperceptron_network.png) The leftmost layer, known as the input layer, consists of a set of \"neurons\" $\\{x_i | x_1, x_2, ..., x_m\\}$ representing the input features (e.g., weighted words). Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation $w_1x_1 + w_2x_2 + ... + w_mx_m$, followed by a non-linear activation function $g(\\cdot):R \\rightarrow R$ - like the logistic or hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_nn = sklearn.neural_network.MLPClassifier()\n",
    "clf_nn.fit(np.stack(train_redditDf['vect'], axis=0), train_redditDf['category'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.evaluateClassifier(clf_nn, test_redditDf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotConfusionMatrix(clf_nn, test_redditDf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lucem_illud_2020.plotregions(clf_nn, test_redditDf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It performs very well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 5*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform a neural network classification and calculate relevant metrics (e.g., precision, recall, the F-measure, and AUC). How does this classify relevant to *k*-nearest neighbor, Naive Bayes, logistic and decision-tree approaches?\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}