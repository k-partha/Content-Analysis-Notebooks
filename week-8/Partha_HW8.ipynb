{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Week - 8 - Deep Neural Nets and Text\n",
    "\n",
    "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
    "\n",
    "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
    "\n",
    "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
    "\n",
    "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
    "\n",
    "Note that if you run the computationally intensive models on your local computer they will take a long time!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch # pip install torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig # pip install tranformers==2.4.1\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lucem_illud #pip install -U git+git://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CoLA Dataset and pre-processing\n",
    "\n",
    "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
    "\n",
    "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -token IDs from BERT’s tokenizer\n",
    "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -segment IDs used to distinguish different sentences\n",
    "    -positional embeddings used to show token position within the sequence\n",
    "\n",
    "\n",
    "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
    "\n",
    "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Introducing Deep Neural Nets\n",
    "\n",
    "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
    "\n",
    "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_lstm2 = Sequential()\n",
    "# model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(LSTM(units))\n",
    "# model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "# model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# history_lstm2 = model_lstm2.fit(input_data_train, labels, epochs=10, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### On with BERT!\n",
    "\n",
    "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading our Models\n",
    "\n",
    "### Train Model\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "### Structure of Fine-Tuning Model\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "### The Fine-Tuning Process\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
    "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
    "\n",
    "## WARNING: SHIFT TO A GPU ENABLED MACHINE (e.g., Google Colab)\n",
    "\n",
    "Note that you only want to run the following code if you have a GPU. Otherwise, rerun the **same** cells we just ran on the Colab file to train your model, download it to your local, and load it by running\n",
    "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model\", num_labels=2)\n",
    "# model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "Once you tune your model on Colab (or on your own machine if you decided to do that instead), you load it here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model\", num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NOTE\n",
    "\n",
    "This results from this notebook (and the corresponding one on Colab) were used for the coding presentation. As my project involves fitting four models for every task, not just one, and because the models can be very big (my machine lacks space and becomes very slow), I have not included all the models here from my coding presentation analysis (which was all performed mostly on Colab).\n",
    "\n",
    "Also, the example code preceding the exercises in the original notebook is omitted from the rest of notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The classification task performed through this notebook involved predicted personality labels from the Twitter likes dataset\n",
    "I've been working on in past assignments. Some key results from the classification task are:\n",
    "\n",
    "- The LSTM performs poorly across all the four binary variables of interest in the prediction task.\n",
    " Visualized below is classification for the variable 'intuitive'. As you can see, it performs very slightly better than chance.\n",
    "- The BERT-based classifier performed much better on this front - achieving an accuracy of nearly 2/3 rds on the same task, on the validation set. (Only the Matthews coefficient (0.33) is shown below, the validation accuracy is from Colab.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "df = pd.read_csv('personality_likes_raw.csv', encoding='cp1252', index_col = 0)\n",
    "\n",
    "df, test_df = train_test_split(df, test_size= 0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.text.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.intuitive.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', '/', '?', '.', '.', '.', 'ix', '##fp', ':', '(', ')', 'https', ':', '/', '/', 't', '.', 'co', '/', 'f', '##zu', '##sea', '##pl', '##ag', ',', ',', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'ox', '##ao', '##yi', '##2', '##v', '##v', '##5', '(', 'yi', '##zh', '##eng', 'ke', ')', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'co', '##9', '##8', '##w', '##j', '##r', '##8', '##t', '##0', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', '2', '##sz', '##5', '##rl', '##0', '##h', '##f', '##n', '3', ':', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', '4', '##m', '##v', '##ho', '##ca', '##z', '##fk', '##1', '?', '.', '.', '.', '2', '.', '.', '.', '.', '.', '.', '.', '.', '.', '3', '3', 'https', ':', '/', '/', 't', '.', 'co', '/', 'y', '##6', '##h', '##8', '##b', '##pp', '##9', '##by', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'b', '##5', '##dm', '##i', '##9', '##sil', '##k', '@', 'he', '##hing', '_', 'is', '##fp', '.', '.', '.', '.', '.', '.', '@', 'he', '##hing', '_', 'is', '##fp', '!', '!', '!', '!', '!', 'https', ':', '/', '/', 't', '.', 'co', '/', 'z', '##cm', '##n', '##3', '##gt', '##hr', '##8', '@', 'he', '##hing', '_', 'is', '##fp', '?', '?', '#', '_', '_', '_', '_', 'rt', '153', '##mb', '##ti', 'is', '##fp', '-', 't', '51', '%', '55', '%', '51', '%', '51', '%', '58', '%', 'is', '##fp', '-', 'a', '51', '%', '55', '%', '51', '%', '51', '%', '58', '%', '-', 'a', ',', '-', 't', '@', 'he', '##hing', '_', 'is', '##fp', '.', '.', '.', ',', ',', ',', '!', '@', 'he', '##hing', '_', 'is', '##fp', ',', '.', '.', ',', '.', '.', '.', '.', '.', '2', ',', '!', '6', '2', '!', '!', 'https', ':', '/', '/', 't', '.', 'co', '/', 'j', '##z', '##5', '##y', '##8', '##j', '##vd', '##z', '##k', '1', '(', ')', '2', 'https', ':', '/', '/', 't', '.', 'co', '/', 'k', '##pt', '##jk', '##a', '##9', '##p', '##q', '##f', '.', '.', '.', '.', '.', '.', '.', '.', '.', '(', '‘', '’', '‘', '’', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'no', '##1', '##ji', '##dic', '##h', '##v', 'in', '##f', '_', '!', 'https', ':', '/', '/', 't', '.', 'co', '/', '4', '##xy', '##z', '##rma', '##42', '##l', '.', '.', '.', '.', '#', '.', '.', '.', '.', '.', '.', '.', '.', '‘', '’', 'https', ':', '/', '/', 't', '.', 'co', '/', 'i', '##0', '##2', '##x', '##u', '##0', '##3', '##ag', '##j', '.', '.', '.', '.', ',', ',', 'in', '##fp', '-', 'a', '.', '.', '?', '?', '?', '&', 'lt', ';', '&', 'lt', ';', '!', '!', '!', '?', '!', '?', '!', '?', '!', '?', '!', 'https', ':', '/', '/', 't', '.', 'co', '/', 'ac', '##x', '##7', '##u', '##8', '##m', '##wk', '##g', '?', '?', '!', '!', '#', 'mb', '##ti', '_', '#', 'is', '##fp', '#', ',', ',', ',', '(', '.', '.', ')', '.', '.', '(', '.', '.', '.', ')', '.', '+', ')', '.', '.', '.', ',', '.', '.', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'f', '##8', '##ox', '##6', '##nts', '##r', '##j', '.', '.', '.', '?', '.', '.', '.', '.', '.', '?', '.', '.', '.', 'e', '.', '.', '.', '.', '.', '.', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'q', '##0', '##x', '##n', '##v', '##m', '##rf', '##sp', '.', '.', '.', '?', '?', '.', '.', '.', '#', '_', '_', '_', '180', '%', '1', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 're', '##ao', '##q', '##gc', '##v', '##2', '##g', '.', '.', 'https', ':', '/', '/', 't', '.', 'co', '/', 'k', '##j', '##y', '##yk', '##i', '##1', '##x', '##l', '##w', '?', '.', '.', '.', '.', '.', '.', '.', '#', 'mb', '##ti', 'https', ':', '/', '/', 't', '.', 'co', '/', '0', '##z', '##jn', '##1', '##rd', '##7', '##q', '##w', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "labels = df.intuitive.values\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39/39 [==============================] - 8s 83ms/step - loss: 0.6934 - accuracy: 0.4921\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 3s 82ms/step - loss: 0.7103 - accuracy: 0.5596\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 3s 86ms/step - loss: 0.6084 - accuracy: 0.8007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 3s 83ms/step - loss: 0.3745 - accuracy: 0.8970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 3s 83ms/step - loss: 0.0716 - accuracy: 0.9862\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 3s 83ms/step - loss: 0.0129 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 3s 88ms/step - loss: 0.0090 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 3s 82ms/step - loss: 0.0026 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 3s 83ms/step - loss: 0.0014 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 3s 86ms/step - loss: 9.9376e-04 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels,\n",
    "                              epochs=10,batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "preds = [0 if i[0]> i[1] else 1 for i in model_lstm.predict(validation_inputs)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5323741007194245"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(preds, validation_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"intuitive_model\", num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Holdout Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = test_df.text.values\n",
    "sentences = [i[:128] for i in sentences]\n",
    "labels = test_df.intuitive.values\n",
    "\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "\n",
    "prediction_labels = torch.tensor(labels)\n",
    "# Set the batch size.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 154 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict\n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None,\n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 72 of 154 (46.75%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (test_df.intuitive.sum(),\n",
    "                                               len(test_df.intuitive), (test_df.intuitive.sum() / len(test_df.intuitive) * 100.0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for pred in predictions:\n",
    "\n",
    "  prediction += [0 if x[0]>x[1] else 1 for x in pred]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 0], dtype=int64)"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.argmax(predictions[0], axis=1).flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    " ##   print(pred_labels_i)\n",
    "\n",
    "\n",
    "    # Calculate and store the coef for this batch.\n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
    "    matthews_set.append(matthews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.325\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some useful functions for later:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)\n",
    "    token_vecs = []\n",
    "    \n",
    "    for embedding in word_embeddings[0]:\n",
    "        cat_vec = embedding.detach().numpy()\n",
    "        token_vecs.append(cat_vec)\n",
    "        \n",
    "    if method == \"average\":\n",
    "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
    "    if method == \"model\":\n",
    "        sentence_embedding = sentence_embeddings\n",
    "    # do something\n",
    "    return sentence_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them. \n",
    "https://github.com/huggingface/transformers#quick-tour-of-pipelines\n",
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Transformers Pipelines\n",
    "\n",
    "We use the sentiment analysis pipeline to calculate the average sentiments expressed across each of the four personality traits.\n",
    "Interestingly, the most major difference in expression of sentiment was seen across the Judging/Perceiving variable -> Judgers were 50% more likely to like tweets expressing positive sentiment.\n",
    "Feeling preference individuals were about 16% more likely to like positive sentiment tweets than those with a thinking preference.\n",
    "\n",
    "There is little difference across I/E. Taken together, these results are interesting and worth investigating further. BERT generates high quality\n",
    "sentiment scores, so these should be highly insightful. Prior methods would either be too slow (manual) or inacccurate (other older NLP pipelines).\n",
    "\n",
    "The feature extraction pipeline is less useful for the analysis of the twitter personality dataset; majority of the analysis focusses on the sentiment scores and QA analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/230 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b1a18c0888447c48ee220de6b59a3f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json' to download model card file.\n",
      "Creating an empty model card.\n"
     ]
    }
   ],
   "source": [
    "##Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "def trunc(x):\n",
    "    return x[:200]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "i_text = df[df['extravert'] == 0]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()\n",
    "e_text = df[df['extravert'] == 1]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "n_text = df[df['intuitive'] == 1]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()\n",
    "s_text = df[df['intuitive'] == 0]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "f_text = df[df['thinking'] == 0]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()\n",
    "t_text = df[df['thinking'] == 1]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "p_text = df[df['judging'] == 0]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()\n",
    "j_text = df[df['judging'] == 1]['text'].sample(400, random_state = 42).map(lambda x: trunc(x)).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "def percPositive(x):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for i in x:\n",
    "        sen = nlp_sentiment(i)\n",
    "        if sen[0]['label'] == 'POSITIVE':\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    return 100 * pos/(pos + neg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.5\n",
      "38.25\n"
     ]
    }
   ],
   "source": [
    "print(percPositive(i_text))\n",
    "print(percPositive(e_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.75\n",
      "35.5\n"
     ]
    }
   ],
   "source": [
    "print(percPositive(n_text))\n",
    "print(percPositive(s_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.75\n",
      "41.5\n"
     ]
    }
   ],
   "source": [
    "print(percPositive(t_text))\n",
    "print(percPositive(f_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.25\n",
      "31.75\n"
     ]
    }
   ],
   "source": [
    "print(percPositive(j_text))\n",
    "print(percPositive(p_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# QA\n",
    "\n",
    "Each of the types were asked the question 'What do I like to do?' with the liked tweets given as context.\n",
    "There were some interesting (and disturbing) answers. Some of these were sort of representative of the personality type, most were not.\n",
    "\n",
    "Types were also asked 'I love to?'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/230 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2f7386cdeb14b18850f22c89a10efb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-distilled-squad-modelcard.json' to download model card file.\n",
      "Creating an empty model card.\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "nlp_question = pipeline('question-answering')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.00410427773607816,\n 'start': 4476,\n 'end': 4500,\n 'answer': 'looked at the show wiki,'}"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(i_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.00636751323047946,\n 'start': 1483,\n 'end': 1502,\n 'answer': 'therapy litigation?'}"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(e_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.2881278074151634,\n 'start': 1070,\n 'end': 1118,\n 'answer': 'listen to eye of the tiger and get too riled up?'}"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(n_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.37193972870325354,\n 'start': 727,\n 'end': 744,\n 'answer': 'playing twister.I'}"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(s_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.1814056794085097,\n 'start': 284,\n 'end': 308,\n 'answer': 'shoot all these Blazers.'}"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(t_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 14.93it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.6977528588004631,\n 'start': 3904,\n 'end': 3933,\n 'answer': 'drive out what he represents.'}"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(f_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.47051690860325834,\n 'start': 4527,\n 'end': 4551,\n 'answer': 'shutter such a mind fuck'}"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(j_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.0928724932157099,\n 'start': 1985,\n 'end': 2000,\n 'answer': 'sea shanties...'}"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'What do I like to do ?',\n",
    "'context': ' '.join(p_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I love to ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/230 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b25c3d70d63848b99e8af213e780f5c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-distilled-squad-modelcard.json' to download model card file.\n",
      "Creating an empty model card.\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "nlp_question = pipeline('question-answering')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 12.66it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.00014757042162302086,\n 'start': 3828,\n 'end': 3866,\n 'answer': 'week.Spreading love on your timeline.'}"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(i_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 12.99it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 3.483897437971876e-05,\n 'start': 1139,\n 'end': 1162,\n 'answer': '“America the Beautiful”'}"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(e_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 13.52it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 9.85067531132644e-05,\n 'start': 4719,\n 'end': 4735,\n 'answer': 'Love you, Miche.'}"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(n_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 12.83it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.0245091108148372,\n 'start': 1005,\n 'end': 1014,\n 'answer': '@Amarisse'}"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(s_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 14.50it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.02472686240653843,\n 'start': 2254,\n 'end': 2309,\n 'answer': 'We are head over heels in love with this little nugget!'}"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(t_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 14.50it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.00020052910111982222,\n 'start': 2974,\n 'end': 3012,\n 'answer': 'up.Thank you to the frontline workers,'}"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(f_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.0004658211877896501,\n 'start': 2498,\n 'end': 2509,\n 'answer': 'gorgeous...'}"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(j_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'score': 0.001363327064610731,\n 'start': 3630,\n 'end': 3671,\n 'answer': 'https://t.co/NIPWZECKrO@LOVE_ME_T__T'}"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "'question': 'I love to?',\n",
    "'context': ' '.join(p_text)[:5000]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NOTE\n",
    "\n",
    "While the RoBERTa models were successfully trained on the data and ran on my machine as well as on Colab, the GPT models would not run on my machine. I was not able to solve this issue even after discussing this very issue with Bhargav.\n",
    "However, the GPT model was trained successfully ('GPT_N' and 'GPT_S') and ran on Colab (with the very same code below), the results of which are contained in the coding presentation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-10 18:33:16.941428: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-03-10 18:33:16.941786: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "03/10/2021 18:33:20 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False\n",
      "03/10/2021 18:33:20 - INFO - transformers.tokenization_utils -   Model name 'output_gpt_N' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output_gpt_N' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/10/2021 18:33:20 - INFO - transformers.tokenization_utils -   Didn't find file output_gpt_N\\added_tokens.json. We won't load it.\n",
      "03/10/2021 18:33:20 - INFO - transformers.tokenization_utils -   loading file output_gpt_N\\vocab.json\n",
      "03/10/2021 18:33:20 - INFO - transformers.tokenization_utils -   loading file output_gpt_N\\merges.txt\n",
      "03/10/2021 18:33:20 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/10/2021 18:33:20 - INFO - transformers.tokenization_utils -   loading file output_gpt_N\\special_tokens_map.json\n",
      "03/10/2021 18:33:20 - INFO - transformers.tokenization_utils -   loading file output_gpt_N\\tokenizer_config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"run_generation.py\", line 295, in <module>\n",
      "    main()\n",
      "  File \"run_generation.py\", line 220, in main\n",
      "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 309, in from_pretrained\n",
      "    return cls._from_pretrained(*inputs, **kwargs)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 458, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_gpt2.py\", line 125, in __init__\n",
      "    super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token, **kwargs)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 253, in __init__\n",
      "    assert isinstance(value, str)\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!python run_generation.py --model_type=gpt2 --model_name_or_path=output_gpt_N"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-10 18:53:00.937752: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-03-10 18:53:00.938893: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "03/10/2021 18:53:07 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False\n",
      "03/10/2021 18:53:07 - INFO - transformers.tokenization_utils -   Model name 'GPT_S' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'GPT_S' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/10/2021 18:53:07 - INFO - transformers.tokenization_utils -   Didn't find file GPT_S\\added_tokens.json. We won't load it.\n",
      "03/10/2021 18:53:07 - INFO - transformers.tokenization_utils -   loading file GPT_S\\vocab.json\n",
      "03/10/2021 18:53:07 - INFO - transformers.tokenization_utils -   loading file GPT_S\\merges.txt\n",
      "03/10/2021 18:53:07 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/10/2021 18:53:07 - INFO - transformers.tokenization_utils -   loading file GPT_S\\special_tokens_map.json\n",
      "03/10/2021 18:53:07 - INFO - transformers.tokenization_utils -   loading file GPT_S\\tokenizer_config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"run_generation.py\", line 295, in <module>\n",
      "    main()\n",
      "  File \"run_generation.py\", line 220, in main\n",
      "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 309, in from_pretrained\n",
      "    return cls._from_pretrained(*inputs, **kwargs)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 458, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_gpt2.py\", line 125, in __init__\n",
      "    super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token, **kwargs)\n",
      "  File \"C:\\Users\\Partha\\PycharmProjects\\CompAnalysis\\venv\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 253, in __init__\n",
      "    assert isinstance(value, str)\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!python run_generation.py --model_type=gpt2 --model_name_or_path=GPT_S"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Three texts were fed into two RoBERTa models (low and high openness individuals) to understand how they associate a word more commonly used with ordinary meanings related to a usage in a more abstract domain.\n",
    "This fits with the theme of investigating low and high openness individuals - high openness individuals may show more association to the abstract theme usage of the words in question.\n",
    "\n",
    "The results do not show significant differences between the plots, so it is not possible for us to test the hypotheses that we laid out above. We may need more powerful tests/more data to dsitinguish the differences between individuals that we hypothesize.\n",
    "\n",
    "However some promising signs can be seen in the plots, for example, the association between 'learning' and 'machine' is stronger in the intuitives model as compared to the non intuitives model, which is exactly the kind of effect that we would expect."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "roberta_us_model_embedding = RobertaModel.from_pretrained('Roberta_N')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "text1 = \"I love science, technology, and healthcare!.\" # To understand how low and high openness individuals view healthcare when compared to science/technology.\n",
    "text2 = \"This is the latest ARM CPU balancing on my hand.\" # To see how ARM is related to CPU or a bodily appendage, across high and low openness individuals.\n",
    "text3 = \"Should I train my machine learning model or my dog?\" # To understand how 'train' relates to machines as compared to a  dog, across high and low openness individuals.\n",
    "text4 = \"Was the bog closer to the forest or the house?\" # stock sentence\n",
    "text5 = \"Are you taking the trolley or the train to the grocery market\" # stock sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text3, roberta_us_model_embedding, roberta_us_tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "roberta_gb_model_embedding = RobertaModel.from_pretrained('Roberta_S')\n",
    "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text3, roberta_gb_model_embedding, roberta_gb_tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}